{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Deep Q-Network (DQN)\n",
    "\n",
    "### 0.Background: introduce Asynchronous to DQN\n",
    "\n",
    "Traditional [Deep Q-Networks (DQN)](./vanilla_dqn_lunarlander.ipynb) have been highly successful in solving reinforcement learning tasks by combining Q-learning with deep neural networks. However, they often require significant computational resources and suffer from stability issues during training. To address these challenges, **Asynchronous DQN** introduces the concept of **asynchronous multi-threading**, where multiple agents (or threads) interact with their own copies of the environment in parallel. This approach provides several benefits:\n",
    "\n",
    "1. **Improved computational efficiency**:\n",
    "   - By leveraging multiple threads, the algorithm can make better use of multi-core CPUs, enabling faster training without requiring expensive GPUs.\n",
    "\n",
    "2. **Stabilized training**:\n",
    "   - Asynchronous updates reduce the correlation between consecutive updates, which helps mitigate the instability caused by highly correlated training data in traditional DQN.\n",
    "\n",
    "3. **Enhanced exploration**:\n",
    "   - By assigning different exploration policies to each thread, Asynchronous DQN encourages diverse exploration, leading to better coverage of the state-action space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages\n",
    "\n",
    "In this notebook, we will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# for game rendering\n",
    "import time\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "from plot_utils import plot_scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Training on: {device}\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore Environment\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/lunar_lander.gif\" alt=\"Mountain Car Environment\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "#### Discreate Action Space\n",
    "\n",
    "There are four discrete actions available:\n",
    "\n",
    "- 0: do nothing\n",
    "- 1: fire left orientation engine\n",
    "- 2: fire main engine\n",
    "- 3: fire right orientation engine\n",
    "\n",
    "#### Continuous Observation Space\n",
    "\n",
    "The state is an 8-dimensional vector: \n",
    "\n",
    "- the coordinates of the lander in `x`\n",
    "- the coordinates of the lander in`y`, \n",
    "- linear velocities in `x` \n",
    "- linear velocities in `y`, \n",
    "- its angle, \n",
    "- its angular velocity, and \n",
    "- a booleans that represent whether `left` leg is in contact with the ground or not.\n",
    "- a booleans that represent whether `right` leg is in contact with the ground or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:\n",
      " Continuous (8,)\n",
      " - low: [ -2.5    -2.5   -10.    -10.     -6.283 -10.     -0.     -0.   ]\n",
      " - high: [ 2.5    2.5   10.    10.     6.283 10.     1.     1.   ]\n",
      "Action space:\n",
      " Discrete(4)\n",
      "--------------------------------------------------\n",
      "State space 10 samples:\n",
      "[[-0.084 -1.769 -8.835 -0.046 -5.531 -6.111  0.761  0.113]\n",
      " [-0.632  0.289  9.69   7.462  5.389  2.475  0.936  0.035]\n",
      " [ 1.71   0.825 -7.457 -9.346  6.067 -8.19   0.378  0.124]\n",
      " [ 0.381 -1.564  2.214  6.432 -3.74   4.846  0.945  0.355]\n",
      " [ 1.11   2.259 -7.542 -6.113 -1.376 -4.505  0.875  0.089]\n",
      " [ 0.636  1.213 -6.767  3.942  3.822 -0.947  0.644  0.31 ]\n",
      " [ 0.455 -1.529 -3.578 -8.27  -5.491 -1.412  0.334  0.942]\n",
      " [ 1.691 -0.114 -1.026  9.361 -1.019  7.064  0.686  0.935]\n",
      " [ 1.373  1.163 -3.142 -1.514  0.016  8.337  0.961  0.045]\n",
      " [-2.313 -1.529 -7.885 -2.781  4.575 -3.142  0.809  0.53 ]]\n",
      "Action space 10 samples:\n",
      "[2 2 1 2 0 0 3 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\\n Continuous\", env.observation_space.shape)\n",
    "print(\" - low:\", env.observation_space.low)\n",
    "print(\" - high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\\n\", env.action_space)\n",
    "\n",
    "print(\"-\"*50)\n",
    "# Generate some samples from the state space \n",
    "print(\"State space 10 samples:\")\n",
    "print(np.array([env.observation_space.sample() for i in range(10)]))\n",
    "\n",
    "# Generate some samples from the action space\n",
    "print(\"Action space 10 samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Replay Buffer\n",
    "\n",
    "#### Using Experience Replay Improve Training Stability\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/reply-buffer.png\" alt=\"Mountain Car Environment\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "When an agent interacts with the environment, the sequence of experience tuples can be highly correlated, which poses a risk for naive Q-learning algorithms. These algorithms, when learning sequentially from such correlated data, may lead to unstable updates, causing action values to oscillate or diverge.\n",
    "\n",
    "To address this, a replay buffer is introduced to store experience tuples $(S, A, R, S')$ collected during interactions with the environment. By using **experience replay**, small batches of tuples are randomly sampled from the buffer for training. This random sampling breaks harmful correlations, stabilizes learning, and allows the agent to:  \n",
    "1. Reuse individual experience tuples multiple times.  \n",
    "2. Recall rare events.  \n",
    "3. Make better overall use of past experiences.  \n",
    "\n",
    "Experience replay thus improves the efficiency and stability of the learning process.\n",
    "\n",
    "<span style=\"color: red;\">Update: </span> However, asynchronous methods eliminate the need for a replay buffer. In asynchronous one-step Q-learning and related methods, multiple agents (threads) interact with their own copies of the environment in parallel. This naturally introduces diversity into the data collected by each thread, breaking the correlation between consecutive updates. As a result:\n",
    "\n",
    "The updates from different threads act as a form of implicit decorrelation, reducing the risk of instability.\n",
    "The computational complexity of managing a replay buffer is avoided, making the algorithm simpler and more efficient.\n",
    "Thus, asynchronous mechanisms provide a built-in alternative to experience replay, achieving stability and efficiency without the need for a replay buffer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Q Network\n",
    "\n",
    "#### Using Neural Networks as Approximators for Q values\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/model-view.png\" alt=\"Mountain Car Environment\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "This image shows the transition from a **Traditional Q Table** to a **Parameterized Q Function** in reinforcement learning. \n",
    "\n",
    "The Q table (left) stores discrete Q-values $ Q(s_t, a_t) $ for each state-action pair but struggles with scalability in high-dimensional spaces. The parameterized Q function (right) replaces the table with a neural network $ Q(s_t, a_t; w) $, where $ w $ are the network's parameters.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/model-arch.png\" alt=\"Mountain Car Environment\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "- Define a neural network architecture that maps states to action values $Q(s_t, a_t; w)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden_size=64, seed=42):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.Q = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        actions = self.Q(state)\n",
    "        return actions\n",
    "\n",
    "\n",
    "# test model\n",
    "q_net = QNetwork(8, 4, 16, seed=42)\n",
    "# fake input, by given batch size 4\n",
    "states = torch.rand((4, 8))\n",
    "# fake output size\n",
    "print(q_net(states).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Agent\n",
    "\n",
    "##### How to Learn in DQN\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./images/dqn-gradient-descent.png\" alt=\"Mountain Car Environment\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "This image provides a visual summary of the core theory and update mechanism of **Deep Q-Network (DQN)**, showcasing the key formula derivations from **Q-Learning** to **DQN**, as well as how neural networks are used to approximate and update Q-values.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Core Idea Based on the Bellman Equation**\n",
    "- **Bellman Equation:**\n",
    "  $$\n",
    "  Q^*(s_t, a_t) = R_t + \\gamma \\max_a Q^*(s_{t+1}, a)\n",
    "  $$\n",
    "  - This states that the optimal Q-value for the current state-action pair equals the immediate reward $ R_t $ plus the discounted maximum Q-value of the next state.\n",
    "  - The core objective of DQN is to approximate this equation using a neural network.\n",
    "\n",
    "**2. Derivation from Q-Learning to DQN**\n",
    "- **Q-Learning Update Formula:**\n",
    "  $$\n",
    "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( R_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "  $$\n",
    "  - **TD Target:**\n",
    "    $$\n",
    "    R_t + \\gamma \\max_a Q(s_{t+1}, a)\n",
    "    $$\n",
    "    This represents the target Q-value for the current state, combining the immediate reward $ R_t $ and the maximum Q-value of the next state.\n",
    "  - **Current Value:**\n",
    "    $$\n",
    "    Q(s_t, a_t)\n",
    "    $$\n",
    "    This is the current estimated Q-value.\n",
    "  - **TD Error:**\n",
    "    $$\n",
    "    \\delta_t = \\left( R_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "    $$\n",
    "    This measures the difference between the target Q-value and the current Q-value.\n",
    "\n",
    "- **Neural Network Introduction:**\n",
    "  - To handle continuous state spaces, DQN replaces the traditional Q-table with a neural network $ Q(s, a; w) $, where $ w $ represents the network parameters.\n",
    "  - The goal is to optimize the network so that its output $ Q(s, a; w) $ approximates the true $ Q^*(s, a) $.\n",
    "\n",
    "\n",
    "**3. Loss Function and Gradient Update in DQN**\n",
    "- **Loss Function:**\n",
    "  $$\n",
    "  L(w) = \\frac{1}{2} \\left[ Q(s_t, a_t; w) - Q^*(s_t, a_t) \\right]^2\n",
    "  $$\n",
    "  - Here, $ Q^*(s_t, a_t) $ is the target Q-value (TD Target) calculated using the Bellman equation.\n",
    "  - The loss function minimizes the squared error between the network's output $ Q(s_t, a_t; w) $ and the target Q-value $ Q^*(s_t, a_t) $.\n",
    "\n",
    "- **Gradient Calculation:**\n",
    "  $$\n",
    "  \\nabla_w L(w) = \\left( Q(s_t, a_t; w) - Q^*(s_t, a_t) \\right) \\nabla_w Q(s_t, a_t; w)\n",
    "  $$\n",
    "  - The gradient consists of two parts:\n",
    "    1. The error term: $ Q(s_t, a_t; w) - Q^*(s_t, a_t) $\n",
    "    2. The gradient of the network output: $ \\nabla_w Q(s_t, a_t; w) $\n",
    "\n",
    "- **Weight Update:**\n",
    "  $$\n",
    "  w \\leftarrow w - \\alpha \\nabla_w L(w)\n",
    "  $$\n",
    "  - Using gradient descent, the network parameters $ w $ are updated to reduce the loss function.\n",
    "\n",
    "- **Final DQN Update Formula:**\n",
    "  $$\n",
    "  L(w) = \\frac{1}{2} \\left[ Q(s_t, a_t; w) - (R_t + \\gamma \\max_a Q(s_{t+1}, a; w^-)) \\right]^2\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  w' \\leftarrow w + \\alpha \\left( R_t + \\gamma \\max_a Q(s_{t+1}, a; w^-) - Q(s_t, a_t; w) \\right) \\nabla_w Q(s_t, a_t; w)\n",
    "  $$\n",
    "  - Here, $ w^- $ represents the fixed target network parameters during the learning step, which are used to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size,\n",
    "        action_size,\n",
    "        Q_network,\n",
    "        Q_target_network,\n",
    "        optimizer,\n",
    "        gamma=0.99,\n",
    "        target_update_steps=1000,\n",
    "        update_steps=5,\n",
    "        seed=42,\n",
    "        global_T=None,\n",
    "    ):\n",
    "        \"\"\"Initialize an Agent object.\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.Q = Q_network\n",
    "        self.Q_target = Q_target_network\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.target_update_steps = target_update_steps\n",
    "        self.update_steps = update_steps  # Number of steps after which to apply gradients\n",
    "        self.t_step = 0  # Time step counter for this thread\n",
    "        self.global_T = global_T  # Shared global counter\n",
    "        self.seed = np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # Initialize accumulated gradients as None\n",
    "        self.reset_gradients()\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        \"\"\"Reset accumulated gradients.\"\"\"\n",
    "        self.accumulated_grads = [torch.zeros_like(p) for p in self.Q.parameters()]\n",
    "    \n",
    "    def select_action(self, state, epsilon=0.0):\n",
    "        \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.Q.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.Q(state)\n",
    "        self.Q.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(actions.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Processes a step and learns from the experience.\"\"\"\n",
    "        # Convert experience to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        action = torch.tensor([[action]], dtype=torch.int64).to(device)\n",
    "        reward = torch.tensor([[reward]], dtype=torch.float32).to(device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        done = torch.tensor([[done]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # Perform learning step\n",
    "        loss = self.compute_loss((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Backpropagate loss and accumulate gradients\n",
    "        self.Q.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradients\n",
    "        with torch.no_grad():\n",
    "            for acc_grad, param in zip(self.accumulated_grads, self.Q.parameters()):\n",
    "                acc_grad += param.grad.clone()\n",
    "\n",
    "        # Increment step counters\n",
    "        self.t_step += 1\n",
    "        with self.global_T.get_lock():\n",
    "            self.global_T.value += 1\n",
    "\n",
    "        # Check if it's time to apply accumulated gradients\n",
    "        if self.t_step % self.update_steps == 0 or done.item():\n",
    "            self.apply_gradients()\n",
    "            self.reset_gradients()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.global_T.value % self.target_update_steps == 0:\n",
    "            self.hard_update()\n",
    "        \n",
    "    def compute_loss(self, experience):\n",
    "        \"\"\"Computes the loss for a single experience tuple.\"\"\"\n",
    "        state, action, reward, next_state, done = experience\n",
    "\n",
    "        # Compute TD target using the target network\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = torch.max(self.Q_target(next_state), dim=-1, keepdim=True)[0]\n",
    "            Q_targets = reward + (1 - done) * self.gamma * Q_targets_next\n",
    "\n",
    "        # Compute expected Q values using the local network\n",
    "        Q_expected = torch.gather(self.Q(state), dim=-1, index=action)\n",
    "\n",
    "        # Compute loss (mean squared error)\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        return loss\n",
    "\n",
    "    def apply_gradients(self):\n",
    "        \"\"\"Apply accumulated gradients to the shared network.\"\"\"\n",
    "        for param, acc_grad in zip(self.Q.parameters(), self.accumulated_grads):\n",
    "            param.grad = acc_grad  # Set the accumulated gradients\n",
    "            \n",
    "        # Perform optimizer step\n",
    "        self.optimizer.step()\n",
    "        # Zero the parameter gradients (in case they weren't zeroed)\n",
    "        self.optimizer.zero_grad()\n",
    " \n",
    "    def hard_update(self):\n",
    "        \"\"\"Hard update: θ_target = θ\"\"\"\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train the Agent with Async DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define SharedAdam optimizer\n",
    "class SharedAdam(torch.optim.Adam):\n",
    "    \"\"\"Implements Adam optimizer with shared states.\"\"\"\n",
    "    def __init__(self, params, lr=5e-4):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                # State initialization\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.tensor(0)\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                # Share in memory\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "                state['step'].share_memory_()\n",
    "\n",
    "\n",
    "def worker_dqn(rank, Q, Q_target, optimizer, global_T, global_max_score, env_name, num_episodes=2000, max_t=1000, window=100):\n",
    "    \"\"\"Function to be executed in each worker process.\"\"\"\n",
    "    # Create an environment instance for each worker\n",
    "    env = gym.make(env_name)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Initialize the agent with shared networks and optimizer\n",
    "    agent = Agent(\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        Q_network=Q,\n",
    "        Q_target_network=Q_target,\n",
    "        optimizer=optimizer,\n",
    "        gamma=0.995,\n",
    "        target_update_steps=50,\n",
    "        update_steps=10,\n",
    "        seed=42 + rank,\n",
    "        global_T=global_T\n",
    "    )\n",
    "    \n",
    "    epsilon = 1.0\n",
    "    eps_min = 0.01\n",
    "    eps_decay = 0.999\n",
    "    \n",
    "    scores_window = deque(maxlen=window)\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        with global_max_score.get_lock():\n",
    "            if global_max_score.value >= 200.0:\n",
    "                break\n",
    "            \n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        agent.t_step = 0\n",
    "        agent.reset_gradients()\n",
    "        done = False\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward, done, _, info = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(eps_min, eps_decay * epsilon)\n",
    "        \n",
    "        scores_window.append(total_reward)\n",
    "        mean_score = np.mean(scores_window)\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print(f\"Process {rank}, Episode {i_episode}, Total Reward: {total_reward:.2f}, Average Score: {mean_score:.2f}\")\n",
    "\n",
    "        if len(scores_window) >= window and mean_score >= 200.0:\n",
    "            with global_max_score.get_lock():\n",
    "                global_max_score.value = mean_score\n",
    "                if rank == 0:\n",
    "                    print(f\"\\nEnvironment solved in {i_episode:d} episodes!\\tAverage Score: {mean_score:.2f}\")\n",
    "                    torch.save(agent.Q.state_dict(), \"checkpoint.pth\")\n",
    "            break\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # still working on...\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     import os\n",
    "#     os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "#     try:\n",
    "#         mp.set_start_method('spawn', force=True)\n",
    "#     except RuntimeError:\n",
    "#         pass\n",
    "\n",
    "#     # Environment name\n",
    "#     env_name = 'LunarLander-v3'\n",
    "#     env = gym.make(env_name)\n",
    "#     state_size = env.observation_space.shape[0]\n",
    "#     action_size = env.action_space.n\n",
    "#     env.close()\n",
    "\n",
    "#     # Initialize global networks\n",
    "#     global_Q = QNetwork(state_size, action_size, hidden_size=128).to('cpu')\n",
    "#     global_Q_target = QNetwork(state_size, action_size, hidden_size=128).to('cpu')\n",
    "#     global_Q.share_memory()\n",
    "#     global_Q_target.share_memory()\n",
    "\n",
    "\n",
    "#     # Initialize global optimizer\n",
    "#     global_optimizer = SharedAdam(global_Q.parameters(), lr=5e-4)\n",
    "\n",
    "#     # Global counter T\n",
    "#     global_T = mp.Value('i', 0)\n",
    "\n",
    "#     # Initialize global max score as a shared value\n",
    "#     global_max_score = mp.Value('d', -float('inf'))\n",
    "\n",
    "#     # Number of processes\n",
    "#     num_processes = 8\n",
    "#     num_episodes = 5000\n",
    "\n",
    "#     processes = []\n",
    "#     for rank in range(num_processes):\n",
    "#         p = mp.Process(target=worker_dqn, args=(\n",
    "#             rank, global_Q, global_Q_target, global_optimizer, global_T, global_max_score, env_name, num_episodes))\n",
    "#         p.start()\n",
    "#         processes.append(p)\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.join()\n",
    "#         if p.exitcode != 0:\n",
    "#             print(f\"Process {p.pid} exited with code {p.exitcode}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
